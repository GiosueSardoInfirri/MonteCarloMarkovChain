{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# I load the needed libraries\n",
    "library(dplyr)\n",
    "library(scales)\n",
    "library(GoFKernel)\n",
    "\n",
    "library(mvtnorm)\n",
    "library(gplots)\n",
    "library(coda)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test using a milti-dimensional gaussian with covariance\n",
    "\n",
    "In order to have the most general form of the gaussian, I use the package \"mvtnorm\", standing for \"Multivariate Normal and t Distributions\", which allows me to create a gaussian in many dimension with a covariance on some axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Data for the check of the effectiveness of the package\n",
    "cov = matrix(c(c(1, 2, 3), c(2, 5, 6), c(3, 6, 9)), ncol=3)\n",
    "x0 = c(1, 3, 5)\n",
    "\n",
    "# Function to be used is very similar to the one used for the one-dimensional gaussian\n",
    "data = rmvnorm(1e5, mean = x0, sigma = cov, method = c(\"eigen\", \"svd\", \"chol\"), pre0.9_9994 = FALSE, checkSymmetry = TRUE)\n",
    "\n",
    "# I restrict the consistency on one of the dimentions in order to see if it really works on the mean \n",
    "hist(data[,3], breaks = sqrt(length(data[,1])))\n",
    "\n",
    "# In order to check the covariance matrix, I plot the 2d histogram \n",
    "hist2d(data[,-1], nbins=sqrt(length(data[,1])), col=c(\"black\", heat.colors(12)), FUN=function(x) length(x))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC using \"mvtnorm\" package\n",
    "\n",
    "It is much faster than using the function that I implemented, and it allows to shape differently the multi-dimensional gaussian. It takes $\\backsim 40$ seconds compared to the other that takes $\\backsim 135$ seconds, more than three times its time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Now I can change the function to generate the new data in the MCMC\n",
    "\n",
    "# Then I create the function to extract the random number\n",
    "generation_s = function (x0, cov) {\n",
    "\n",
    "    # I use the library method to generate the new point\n",
    "    new_value = rmvnorm(1, mean = x0, sigma = cov, method = c(\"eigen\", \"svd\", \"chol\"), pre0.9_9994 = FALSE, checkSymmetry = FALSE)\n",
    "\n",
    "    return(new_value)\n",
    "}\n",
    "\n",
    "# And the one to check the value of the quantiles\n",
    "evaluate_Q = function(x0, cov, point) {\n",
    "\n",
    "    # For the case of the normal distribution\n",
    "    sx = dmvnorm(point, mean = x0, sigma = cov, log = FALSE)\n",
    "    dx = dmvnorm(x0, mean = point, sigma = cov, log = FALSE)\n",
    "\n",
    "    return(sx/dx)\n",
    "}\n",
    "\n",
    "# This function is ment to return the sequence of the samples for a determined function\n",
    "random_steps = function (func_wanted, theta_init, n_samples, sigma, print_accept=FALSE) {\n",
    "\n",
    "    # Initilalizing the parameters\n",
    "    current_theta = theta_init\n",
    "    current_function = func_wanted(theta_init)\n",
    "    samples = matrix(data = NA, nrow = n_samples, ncol = length(theta_init) + 1)\n",
    "\n",
    "    # For statistical purposes\n",
    "    accepted = 0\n",
    "\n",
    "    # Evolution loop\n",
    "    for (n in 1:n_samples) {\n",
    "\n",
    "        # Take a guessed new theta (s in the slides) and evaluate its probability\n",
    "        guessed_theta = generation_s(current_theta, sigma)\n",
    "        guessed_function = func_wanted(guessed_theta)\n",
    "\n",
    "        # Acceptance conditions\n",
    "        Q_ratio = evaluate_Q(current_theta, sigma, guessed_theta)\n",
    "        rho = guessed_function/current_function * Q_ratio\n",
    "        # cat(guessed_theta, guessed_function, Q_ratio, rho)\n",
    "\n",
    "        # And then update the general conditions\n",
    "        if (rho > runif(1)) {\n",
    "            current_theta = guessed_theta\n",
    "            current_function = guessed_function\n",
    "            accepted = accepted + 1\n",
    "        } # else they remain unchanged and then loaded direcctly in the solutions vector\n",
    "\n",
    "        # Saving the generated samples because R doesn't accept two elements in returns\n",
    "        samples[n,] = unlist(c(current_function, current_theta))\n",
    "\n",
    "    }\n",
    "\n",
    "    if(print_accept) {\n",
    "        cat(\"Acceptance rate = \", round(accepted/n_samples*100, 5), '%\\n')\n",
    "    }\n",
    "\n",
    "    return(samples)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Function to sampled from: n-dim gaussian with chosen sigmas and centers\n",
    "posterior_g_inhom = function (theta) {\n",
    "\n",
    "    sigmas = c(1:length(theta))\n",
    "    centers = c(seq(length(theta), 1))\n",
    "\n",
    "    product = 1\n",
    "    for (i in 1:length(theta)) {\n",
    "        product = product * exp(-(theta[i] - centers[i])**2/sigmas[i]**2)\n",
    "    }\n",
    "\n",
    "    return (product)\n",
    "\n",
    "}\n",
    "\n",
    "# The initial parameters are:\n",
    "init = c(1, 2, 3)\n",
    "std = diag(1, 3)\n",
    "burn_in = 0\n",
    "N = as.integer(1e5) + burn_in\n",
    "\n",
    "# Evaluate then the MCMC\n",
    "mcmc_g = random_steps(func_wanted = posterior_g_inhom, theta_init = init, n_samples = N, sigma = std, print_accept=TRUE)\n",
    "\n",
    "# Selecting the sequence after the burn-in\n",
    "mcmc_g = mcmc_g[burn_in:N, ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Printing then the evolution of the chain and the resulting distribution\n",
    "lungh = as.integer(length(mcmc_g[1,]))\n",
    "step = 1e2\n",
    "\n",
    "# Plotting every dimension of the plot\n",
    "par(mfrow=c(lungh-1, 2), oma = c(0, 0, 0, 0))\n",
    "options(repr.plot.width=20, repr.plot.height=6*(lungh-1))\n",
    "\n",
    "for (dim in 2:lungh) {\n",
    "    plot_g = mcmc_g[seq(0, length(mcmc_g[,1]), step), dim]\n",
    "    plot(1:length(plot_g), plot_g, type = 'o', lwd = 1, col = 'black', xlab = 'Iteration', ylab = 'Theta evolution', \n",
    "        main = paste0('Initial value = ', list(init), '     Standard deviation = ', list(std)))\n",
    "    hist(mcmc_g[,dim], breaks = as.integer(sqrt(length(mcmc_g[,1]))), xlab = 'Lambda', ylab = 'Counts',\n",
    "        main = paste('Histogram of the posterior distribution in the dimension', dim-1))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lungh = as.integer(length(mcmc_g[1,]))\n",
    "step = 1e2\n",
    "\n",
    "par(mfrow=c(lungh-1, 1), oma = c(0, 0, 0, 0))\n",
    "options(repr.plot.width=20, repr.plot.height=6*(lungh-1))\n",
    "\n",
    "for (dim in 2:lungh) {\n",
    "    # I ensure that this is now a MCMC\n",
    "    g_chain = as.mcmc(mcmc_g[,dim])\n",
    "\n",
    "    # Then I check the autocorrelation using CODA\n",
    "    lags = seq(1,1e5,100)\n",
    "    auto_g = autocorr(g_chain, lags=lags)\n",
    "\n",
    "    # And finally I plot the autocorrelation evolution:\n",
    "    plot(lags, auto_g, ylim=c(0,1), pch=7, col=\"navy\", xlab=\"lag\", ylab=\"ACF\", cex=1.5, main = paste(\"Autocorrelation behaviour with sigma =\", list(std)))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mcmc_copy = mcmc_g\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC using \"mvtnorm\" package + Gibbs Sampling\n",
    "\n",
    "I only add the possibility of moving along each parameter dimension instead of jumping in all the dimentions at once, in order to see if there would be an improvement also in the mvtnorm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Now I can change the function to generate the new data in the MCMC\n",
    "\n",
    "# Then I create the function to extract the random number\n",
    "generation_s_gibbs = function (x0, cov) {\n",
    "\n",
    "    # I use the library method to generate the new point\n",
    "    new_value = rmvnorm(1, mean = x0, sigma = cov, method = c(\"eigen\", \"svd\", \"chol\"), pre0.9_9994 = FALSE, checkSymmetry = FALSE)\n",
    "\n",
    "    return(new_value)\n",
    "}\n",
    "\n",
    "# And the one to check the value of the quantiles\n",
    "evaluate_Q_gibbs = function(x0, cov, point) {\n",
    "\n",
    "    # For the case of the normal distribution\n",
    "    sx = dmvnorm(point, mean = x0, sigma = cov, log = FALSE)\n",
    "    dx = dmvnorm(x0, mean = point, sigma = cov, log = FALSE)\n",
    "\n",
    "    return(sx/dx)\n",
    "}\n",
    "\n",
    "# This function is ment to return the sequence of the samples for a determined function\n",
    "random_steps_gibbs = function (func_wanted, theta_init, n_samples, sigma, print_accept=FALSE) {\n",
    "\n",
    "    # Initilalizing the parameters\n",
    "    current_theta = theta_init\n",
    "    current_function = func_wanted(theta_init)\n",
    "    dimensions = length(theta_init)\n",
    "    samples = matrix(data = NA, nrow = n_samples, ncol = length(theta_init) + 1)\n",
    "\n",
    "    # For statistical purposes\n",
    "    accepted = 0\n",
    "\n",
    "    # Evolution loop\n",
    "    for (n in 1:n_samples) {\n",
    "\n",
    "        guessed_theta = current_theta\n",
    "\n",
    "        # I then can loop on the dimensions of the distribution to allpy the gibbs sampling\n",
    "        for (dim in 1:dimensions) {\n",
    "\n",
    "            # Take a guessed new theta (s in the slides) and evaluate its probability\n",
    "            guessed_theta[dim] = generation_s_gibbs(current_theta, sigma)[dim]\n",
    "            guessed_function = func_wanted(guessed_theta)\n",
    "\n",
    "            # Acceptance conditions\n",
    "            Q_ratio = evaluate_Q_gibbs(current_theta, sigma, guessed_theta)\n",
    "            rho = guessed_function/current_function * Q_ratio\n",
    "            # cat(guessed_theta, guessed_function, Q_ratio, rho)\n",
    "\n",
    "            # And then update the general conditions\n",
    "            if (rho > runif(1)) {\n",
    "                current_theta = guessed_theta\n",
    "                current_function = guessed_function\n",
    "                accepted = accepted + 1\n",
    "            } # else they remain unchanged and then loaded direcctly in the solutions vector\n",
    "\n",
    "            # Saving the generated samples because R doesn't accept two elements in returns\n",
    "            samples[n,] = unlist(c(current_function, current_theta))\n",
    "        \n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    if(print_accept) {\n",
    "        cat(\"Acceptance rate = \", round(accepted/(n_samples*dimensions)*100, 5), '%\\n')\n",
    "    }\n",
    "\n",
    "    return(samples)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Function to sampled from: n-dim gaussian with chosen sigmas and centers\n",
    "posterior_g_inhom = function (theta) {\n",
    "\n",
    "    sigmas = c(1:length(theta))\n",
    "    centers = c(seq(length(theta), 1))\n",
    "\n",
    "    product = 1\n",
    "    for (i in 1:length(theta)) {\n",
    "        product = product * exp(-(theta[i] - centers[i])**2/sigmas[i]**2)\n",
    "    }\n",
    "\n",
    "    return (product)\n",
    "\n",
    "}\n",
    "\n",
    "# The initial parameters are:\n",
    "init = c(1, 2, 3)\n",
    "std = diag(1, 3)\n",
    "burn_in = 0\n",
    "N = as.integer(1e5) + burn_in\n",
    "\n",
    "# Evaluate then the MCMC\n",
    "mcmc_g = random_steps_gibbs(func_wanted = posterior_g_inhom, theta_init = init, n_samples = N, sigma = std, print_accept=TRUE)\n",
    "\n",
    "# Selecting the sequence after the burn-in\n",
    "mcmc_g = mcmc_g[burn_in:N, ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Printing then the evolution of the chain and the resulting distribution\n",
    "lungh = as.integer(length(mcmc_g[1,]))\n",
    "step = 1e2\n",
    "\n",
    "# Plotting every dimension of the plot\n",
    "par(mfrow=c(lungh-1, 2), oma = c(0, 0, 0, 0))\n",
    "options(repr.plot.width=20, repr.plot.height=6*(lungh-1))\n",
    "\n",
    "for (dim in 2:lungh) {\n",
    "    plot_g = mcmc_g[seq(0, length(mcmc_g[,1]), step), dim]\n",
    "    plot(1:length(plot_g), plot_g, type = 'o', lwd = 1, col = 'black', xlab = 'Iteration', ylab = 'Theta evolution', \n",
    "        main = paste0('Initial value = ', list(init), '     Standard deviation = ', list(std)))\n",
    "    hist(mcmc_g[,dim], breaks = as.integer(sqrt(length(mcmc_g[,1]))), xlab = 'Lambda', ylab = 'Counts',\n",
    "        main = paste('Histogram of the posterior distribution in the dimension', dim-1))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lungh = as.integer(length(mcmc_g[1,]))\n",
    "step = 1e2\n",
    "\n",
    "par(mfrow=c(lungh-1, 1), oma = c(0, 0, 0, 0))\n",
    "options(repr.plot.width=20, repr.plot.height=6*(lungh-1))\n",
    "\n",
    "for (dim in 2:lungh) {\n",
    "    # I ensure that this is now a MCMC\n",
    "    g_chain = as.mcmc(mcmc_g[,dim])\n",
    "\n",
    "    # Then I check the autocorrelation using CODA\n",
    "    lags = seq(1,1e5,100)\n",
    "    auto_g = autocorr(g_chain, lags=lags)\n",
    "\n",
    "    # And finally I plot the autocorrelation evolution:\n",
    "    plot(lags, auto_g, ylim=c(0,1), pch=7, col=\"navy\", xlab=\"lag\", ylab=\"ACF\", cex=1.5, main = paste(\"Autocorrelation behaviour with sigma =\", list(std)))\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and performancies: \n",
    "\n",
    "I use a table in order to present the results:\n",
    "\n",
    "| Function              | Acceptance Rate (%) | Time needed (s) | Coda Autocorrelation  |\n",
    "|---                    |---                  |---              |---                    |\n",
    "| 1D MCMC               | 60                  | 30              | low                   |\n",
    "| 3D MCMC Metropolis    | 48                  | 130             | low                   |\n",
    "| 3D MCMC Gibbs         | 59                  | 135             | low                   |\n",
    "| 3D MCMC mvtnorm       | 50                  | 40              | low                   |\n",
    "| 3D MCMC mvtnorm Gibbs | 57                  | 140             | low                   |\n",
    "\n",
    "These results have been retrieved running the program three times with different initial points looking for a total of 100_000 points using a very wide gaussian for the sampling (unchanged) in order to present the strenghts and weakness of the three methods.\n",
    "\n",
    "- 1D is very performing, but has the limit of the dimensions\n",
    "\n",
    "- 3D Metropolis has a low acceptance rate: the gaussian for the selection of the new point is very wide, resulting in a small acceptance rate\n",
    "- 3D Gibbs returns to the higher acceptance rate found in the 1D case (since it moves in 1D)\n",
    "\n",
    "- 3D mvtnorm gives the possibility of having covariance, but at the cost of time speed and acceptance rate (it is done without Gibbs sampling, in order to spare time and computational resources, otherwise they would scale linearly with the number of dimentions)\n",
    "- 3D mvtnorm Gibbs, as done in the previous case, increases the acceptance rate at the cost of time\n",
    "\n",
    "All of them have a low Autocorrelation due to the very wide sampling ditribution, but unluckily all of them suffer from the same problem: no covariance in the selecting distribution. We want an algorithm that keeps the autocorrelation low, but increases the acceptance rate, shaping the sampling distribution at will in order to archieve these goals\n",
    "\n",
    "#### Attention: I cannot use these algorithms for ADAPTIVE MCMC, because they don't allow to shape properly the multi-dimensional gaussian, therefore I need an algorithm that uses directly the covariance matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
